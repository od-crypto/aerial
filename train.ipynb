{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import ternausnet.models\n",
    "model = ternausnet.models.UNet11(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'datasets/landcover_processed/rotated_crops/'\n",
    "list_dataset = os.listdir(root_path)\n",
    "list_dataset = list(map(lambda x: root_path + x, list_dataset))\n",
    "train_img_list, test_img_list = train_test_split(list_dataset, test_size=0.15)\n",
    "test_img_list = list(filter(lambda x: x.endswith('-0.png'), test_img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23210 983\n"
     ]
    }
   ],
   "source": [
    "print(len(train_img_list), len(test_img_list))\n",
    "with open('train_img_list.json', 'w') as f:\n",
    "    json.dump(train_img_list, f)\n",
    "    \n",
    "with open('test_img_list.json', 'w') as f:\n",
    "    json.dump(test_img_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.train import WaterDataset, viz, train_transform, test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = WaterDataset('train_img_list.json', train_transform)\n",
    "d_val = WaterDataset('test_img_list.json', test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz2(s):\n",
    "    img = (s * torch.tensor([0.229, 0.224, 0.225])[:, None, None] + torch.tensor([0.485, 0.456, 0.406])[:, None,None])\n",
    "    print(img.min(), img.max())\n",
    "    img = (255 * img).transpose(0,1).transpose(1,2).numpy().astype(np.uint8)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(0.6275)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz2(d_val[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.9877, -0.9705, -0.9705,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-0.9877, -0.9705, -0.9705,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-1.0048, -0.9705, -0.9705,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          ...,\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "         [[-0.8102, -0.8277, -0.8452,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-0.7927, -0.8277, -0.8277,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-0.8102, -0.8102, -0.8277,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          ...,\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "         [[-0.5670, -0.5670, -0.5844,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-0.5670, -0.5670, -0.5844,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-0.6018, -0.5844, -0.6018,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          ...,\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]),\n",
       " tensor([[[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_val[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "dl_train = DataLoader(d_train, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "dl_val = DataLoader(d_val, batch_size=4, shuffle=False, num_workers=4)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "groups = {'acccuracy': ['acc', 'val_acc'], 'bce-loss': ['loss', 'val_loss'], 'lake-acc': ['lakeacc', 'val_lakeacc']}\n",
    "\n",
    "plotlosses = PlotLosses(groups=groups)\n",
    "\n",
    "topk_val_losses = {}\n",
    "\n",
    "train_id = '123'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('train step')\n",
    "    \n",
    "    S_loss = 0\n",
    "    S_accuracy = 0\n",
    "    S_lake_accuracy = 0\n",
    "    for idx, (im, gt) in enumerate(dl_train):\n",
    "        im = im.to(device)\n",
    "        gt = gt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(im)\n",
    "        L = loss(pred, gt)\n",
    "        assert pred.shape == gt.shape\n",
    "        treshold = 0.5\n",
    "        accuracy = ((pred > threshold) != gt.bool()).sum().item() / pred.numel()\n",
    "        lake_accuracy = ((gt == 1) & (pred > threshold)).sum().item() / (gt == 1).sum().item()\n",
    "        #(pred[[]] - gt\n",
    "        # print(L.item())\n",
    "              #(pred > 0.5).sum())\n",
    "        S_loss += L.item()\n",
    "        S_accuracy += accuracy\n",
    "        S_lake_accuracy += lake_accuracy\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = S_loss / (idx + 1)\n",
    "    train_accuracy = S_accuracy/ (idx + 1)\n",
    "    train_lake_accuracy = S_lake_accuracy/ (idx + 1)\n",
    "    \n",
    "        #if (idx % 10) == 0:\n",
    "        #    pass # eval\n",
    "        \n",
    "    print('eval step')\n",
    "    S_loss = 0\n",
    "    S_accuracy = 0\n",
    "    S_lake_accuracy = 0\n",
    "    for idx, (im, gt) in enumerate(dl_val):\n",
    "        im = im.to(device)\n",
    "        gt = gt.to(device)\n",
    "        pred = model(im)\n",
    "        L = loss(pred, gt)\n",
    "        # print(L.item())\n",
    "        S_loss += L.item()\n",
    "        treshold = 0.5\n",
    "        accuracy = ((pred > threshold) != gt.bool()).sum().item() / pred.numel()\n",
    "        lake_accuracy = ((gt == 1) & (pred > threshold)).sum().item() / (gt == 1).sum().item()\n",
    "        S_accuracy += accuracy\n",
    "        S_lake_accuracy += lake_accuracy\n",
    "    val_loss = S_loss / (idx + 1)\n",
    "    val_accuracy = S_accuracy/ (idx + 1)\n",
    "    val_lake_accuracy = S_lake_accuracy/ (idx + 1)\n",
    "    \n",
    "    plotlosses.update({'val_loss': val_loss, 'loss': train_loss, 'val_acc': val_accuracy, 'acc': train_accuracy, 'lakeacc': train_lake_accuracy, 'val_lakeacc': val_lake_accuracy})\n",
    "    plotlosses.send()\n",
    "    \n",
    "    if (len(topk_val_losses) < 5) or (val_loss < max(topk_val_losses.keys())):\n",
    "        if (len(topk_val_losses) > 0) and (val_loss < max(topk_val_losses.keys())):\n",
    "            argmin = max(topk_val_losses.keys())\n",
    "            fname = topk_val_losses[argmin]\n",
    "            os.remove(fname)\n",
    "            del topk_val_losses[argmin]\n",
    "        topk_val_losses[val_loss] = f'model-{train_id}-{epoch}.pth'\n",
    "        torch.save(model.state_dict(), f'model-{train_id}-{epoch}.pth')\n",
    "        \n",
    "torch.save(model.state_dict(), 'model-latest.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
